# helper_mcp_pages.py
# å„ãƒšãƒ¼ã‚¸ã‚¯ãƒ©ã‚¹ã®å®Œå…¨å®Ÿè£…
# ç›´æ¥ã‚¯ã‚¨ãƒªã€ãƒ‡ãƒ¼ã‚¿åˆ†æã€è¨­å®šãƒšãƒ¼ã‚¸ã‚’å«ã‚€

import streamlit as st
import pandas as pd
import requests
import redis
import sqlalchemy
import json
import traceback
import os
from datetime import datetime
from typing import Dict, Any, List
from helper_mcp import PageManager, ServerStatusManager


# ==================================================
# ç›´æ¥ã‚¯ã‚¨ãƒªãƒšãƒ¼ã‚¸
# ==================================================
class DirectQueryPage(PageManager):
    """ç›´æ¥ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªãƒšãƒ¼ã‚¸"""

    def render(self):
        st.header("ğŸ“Š ç›´æ¥ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒª")

        query_type = st.selectbox("ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—ã‚’é¸æŠ",
                                  ["Redis", "PostgreSQL", "Elasticsearch", "Qdrant"])

        if query_type == "Redis":
            self._render_redis_query()
        elif query_type == "PostgreSQL":
            self._render_postgresql_query()
        elif query_type == "Elasticsearch":
            self._render_elasticsearch_query()
        elif query_type == "Qdrant":
            self._render_qdrant_query()

    def _render_redis_query(self):
        """Redisã‚¯ã‚¨ãƒªæ©Ÿèƒ½"""
        st.subheader("ğŸ”´ Redis ã‚¯ã‚¨ãƒª")

        # äº‹å‰å®šç¾©ã•ã‚ŒãŸã‚¯ã‚¨ãƒª
        redis_queries = {
            "å…¨ã‚­ãƒ¼è¡¨ç¤º"  : "KEYS *",
            "ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°": "KEYS session:*",
            "ã‚«ã‚¦ãƒ³ã‚¿ä¸€è¦§": "KEYS counter:*",
            "ã‚«ãƒ†ã‚´ãƒªè¡¨ç¤º": "SMEMBERS categories:all",
            "æ¤œç´¢å±¥æ­´"    : "LRANGE search:recent 0 -1"
        }

        col1, col2 = st.columns([1, 2])

        with col1:
            st.write("**ã‚¯ã‚¤ãƒƒã‚¯ã‚¯ã‚¨ãƒª:**")
            for name, cmd in redis_queries.items():
                if st.button(name, key=f"redis_{name}"):
                    st.session_state.redis_command = cmd

        with col2:
            redis_command = st.text_input(
                "Redisã‚³ãƒãƒ³ãƒ‰",
                value=getattr(st.session_state, 'redis_command', 'KEYS *'),
                key="redis_input"
            )

        if st.button("å®Ÿè¡Œ", key="redis_exec"):
            redis_manager = self.status_manager.get_manager('Redis')
            status = redis_manager.check_connection()

            if "ğŸŸ¢" in status["status"]:
                try:
                    r = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
                    result = self._execute_redis_command(r, redis_command)
                    if result is not None:
                        if isinstance(result, list):
                            st.json(result)
                        else:
                            st.code(str(result))
                except Exception as e:
                    st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
            else:
                st.warning("Redis ã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“")

    def _execute_redis_command(self, redis_client, command: str):
        """Redis ã‚³ãƒãƒ³ãƒ‰ã®å®‰å…¨ãªå®Ÿè¡Œ"""
        cmd_parts = command.strip().split()
        cmd = cmd_parts[0].upper()

        if cmd == "KEYS":
            pattern = cmd_parts[1] if len(cmd_parts) > 1 else "*"
            return sorted(redis_client.keys(pattern))
        elif cmd == "GET":
            if len(cmd_parts) > 1:
                return redis_client.get(cmd_parts[1])
            else:
                st.error("GET ã‚³ãƒãƒ³ãƒ‰ã«ã¯ã‚­ãƒ¼ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        elif cmd == "HGETALL":
            if len(cmd_parts) > 1:
                return redis_client.hgetall(cmd_parts[1])
            else:
                st.error("HGETALL ã‚³ãƒãƒ³ãƒ‰ã«ã¯ã‚­ãƒ¼ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        elif cmd == "SMEMBERS":
            if len(cmd_parts) > 1:
                return sorted(list(redis_client.smembers(cmd_parts[1])))
            else:
                st.error("SMEMBERS ã‚³ãƒãƒ³ãƒ‰ã«ã¯ã‚­ãƒ¼ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        elif cmd == "LRANGE":
            if len(cmd_parts) >= 4:
                key = cmd_parts[1]
                start = int(cmd_parts[2])
                stop = int(cmd_parts[3])
                return redis_client.lrange(key, start, stop)
            else:
                st.error("LRANGE ã‚³ãƒãƒ³ãƒ‰ã®å½¢å¼: LRANGE key start stop")
        else:
            st.error(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ã‚³ãƒãƒ³ãƒ‰ã§ã™: {cmd}")
            st.info("ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‚³ãƒãƒ³ãƒ‰: KEYS, GET, HGETALL, SMEMBERS, LRANGE")
        return None

    def _render_postgresql_query(self):
        """PostgreSQLã‚¯ã‚¨ãƒªæ©Ÿèƒ½"""
        st.subheader("ğŸŸ¦ PostgreSQL ã‚¯ã‚¨ãƒª")

        # äº‹å‰å®šç¾©ã•ã‚ŒãŸã‚¯ã‚¨ãƒª
        pg_queries = {
            "å…¨é¡§å®¢"    : "SELECT * FROM customers ORDER BY id;",
            "æ±äº¬ã®é¡§å®¢": "SELECT * FROM customers WHERE city = 'æ±äº¬';",
            "æœ€æ–°æ³¨æ–‡"  : "SELECT o.*, c.name FROM orders o JOIN customers c ON o.customer_id = c.id ORDER BY o.order_date DESC LIMIT 5;",
            "å£²ä¸Šçµ±è¨ˆ"  : "SELECT product_name, SUM(price * quantity) as total_sales FROM orders GROUP BY product_name ORDER BY total_sales DESC;",
            "å•†å“åœ¨åº«"  : "SELECT name, stock_quantity, price FROM products ORDER BY stock_quantity DESC;"
        }

        col1, col2 = st.columns([1, 2])

        with col1:
            st.write("**ã‚¯ã‚¤ãƒƒã‚¯ã‚¯ã‚¨ãƒª:**")
            for name, sql in pg_queries.items():
                if st.button(name, key=f"pg_{name}"):
                    st.session_state.sql_query = sql

        with col2:
            sql_query = st.text_area(
                "SQLã‚¯ã‚¨ãƒª",
                value=getattr(st.session_state, 'sql_query', 'SELECT * FROM customers LIMIT 5;'),
                height=100,
                key="pg_input"
            )

        if st.button("å®Ÿè¡Œ", key="pg_exec"):
            pg_manager = self.status_manager.get_manager('PostgreSQL')
            status = pg_manager.check_connection()

            if "ğŸŸ¢" in status["status"]:
                try:
                    if not sql_query.strip().upper().startswith('SELECT'):
                        st.error("å®‰å…¨æ€§ã®ãŸã‚ã€SELECTã‚¯ã‚¨ãƒªã®ã¿å®Ÿè¡Œã§ãã¾ã™")
                    else:
                        engine = sqlalchemy.create_engine(os.getenv('PG_CONN_STR'))
                        df = pd.read_sql(sql_query, engine)

                        if len(df) > 0:
                            st.dataframe(df, use_container_width=True)

                            # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
                            csv = df.to_csv(index=False).encode('utf-8-sig')
                            st.download_button(
                                label="ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                data=csv,
                                file_name=f"query_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                                mime='text/csv'
                            )
                        else:
                            st.info("ã‚¯ã‚¨ãƒªã®çµæœã¯ç©ºã§ã—ãŸ")

                        engine.dispose()
                except Exception as e:
                    st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
            else:
                st.warning("PostgreSQL ã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“")

    def _render_elasticsearch_query(self):
        """Elasticsearchã‚¯ã‚¨ãƒªæ©Ÿèƒ½"""
        st.subheader("ğŸŸ¡ Elasticsearch ã‚¯ã‚¨ãƒª")

        search_term = st.text_input("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "Python")
        search_field = st.selectbox("æ¤œç´¢å¯¾è±¡ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰", ["å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰", "title", "content", "category", "author"])

        if st.button("æ¤œç´¢å®Ÿè¡Œ", key="es_exec"):
            es_manager = self.status_manager.get_manager('Elasticsearch')
            status = es_manager.check_connection()

            if "ğŸŸ¢" in status["status"]:
                hits = es_manager.search_articles(search_term, search_field)

                if hits:
                    st.success(f"ğŸ¯ {len(hits)}ä»¶ã®è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")

                    for hit in hits:
                        article = hit['_source']
                        score = hit['_score']

                        with st.expander(f"ğŸ“ {article['title']} (ã‚¹ã‚³ã‚¢: {score:.2f})"):
                            col1, col2 = st.columns([3, 1])

                            with col1:
                                st.write(f"**å†…å®¹:** {article['content']}")

                                # ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤º
                                if 'highlight' in hit:
                                    st.write("**ãƒã‚¤ãƒ©ã‚¤ãƒˆ:**")
                                    for field, highlights in hit['highlight'].items():
                                        for highlight in highlights:
                                            st.markdown(f"â€¢ {highlight}", unsafe_allow_html=True)

                            with col2:
                                st.metric("é–²è¦§æ•°", f"{article['view_count']:,}")
                                st.write(f"**è‘—è€…:** {article['author']}")
                                st.write(f"**ã‚«ãƒ†ã‚´ãƒª:** {article['category']}")
                                st.write(f"**å…¬é–‹æ—¥:** {article['published_date']}")
                                st.write(f"**ã‚¿ã‚°:** {', '.join(article['tags'])}")
                else:
                    st.info(f"'{search_term}' ã«é–¢ã™ã‚‹è¨˜äº‹ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            else:
                st.warning("Elasticsearch ã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“")

    def _render_qdrant_query(self):
        """Qdrantã‚¯ã‚¨ãƒªæ©Ÿèƒ½"""
        st.subheader("ğŸŸ  Qdrant ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢")

        st.info("ğŸ’¡ å®Ÿéš›ã®ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«ã¯åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ã§ã™ãŒã€ã“ã“ã§ã¯ãƒ†ã‚¹ãƒˆç”¨ã®æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™")

        col1, col2 = st.columns(2)

        with col1:
            search_category = st.selectbox("ã‚«ãƒ†ã‚´ãƒªã§æ¤œç´¢",
                                           ["å…¨ã¦", "ã‚¨ãƒ¬ã‚¯ãƒˆãƒ­ãƒ‹ã‚¯ã‚¹", "ã‚­ãƒƒãƒãƒ³å®¶é›»", "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³", "ã‚¹ãƒãƒ¼ãƒ„"])
            price_range = st.slider("ä¾¡æ ¼å¸¯", 0, 100000, (0, 100000), step=1000)

        with col2:
            limit = st.number_input("å–å¾—ä»¶æ•°", min_value=1, max_value=20, value=5)

        if st.button("æ¤œç´¢å®Ÿè¡Œ", key="qdrant_exec"):
            qdrant_manager = self.status_manager.get_manager('Qdrant')
            status = qdrant_manager.check_connection()

            if "ğŸŸ¢" in status["status"]:
                try:
                    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã®æ§‹ç¯‰
                    filter_conditions = []

                    if search_category != "å…¨ã¦":
                        filter_conditions.append({
                            "key"  : "category",
                            "match": {"value": search_category}
                        })

                    filter_conditions.extend([
                        {"key": "price", "range": {"gte": price_range[0]}},
                        {"key": "price", "range": {"lte": price_range[1]}}
                    ])

                    # æ¤œç´¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
                    search_request = {
                        "filter"      : {
                            "must": filter_conditions
                        },
                        "limit"       : limit,
                        "with_payload": True
                    }

                    response = requests.post(
                        'http://localhost:6333/collections/product_embeddings/points/search',
                        json=search_request,
                        headers={'Content-Type': 'application/json'}
                    )

                    if response.status_code == 200:
                        data = response.json()
                        if 'result' in data and data['result']:
                            points = data['result']

                            st.success(f"ğŸ¯ {len(points)}ä»¶ã®å•†å“ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")

                            # å•†å“ä¸€è¦§è¡¨ç¤º
                            products = []
                            for point in points:
                                product = point['payload']
                                product['id'] = point['id']
                                if 'score' in point:
                                    product['similarity_score'] = point['score']
                                products.append(product)

                            df_results = pd.DataFrame(products)
                            st.dataframe(df_results, use_container_width=True)

                            # å•†å“è©³ç´°ã‚«ãƒ¼ãƒ‰
                            for product in products:
                                with st.expander(f"ğŸ›ï¸ {product['name']} - Â¥{product['price']:,}"):
                                    col1, col2 = st.columns([2, 1])

                                    with col1:
                                        st.write(f"**èª¬æ˜:** {product['description']}")
                                        st.write(f"**ã‚«ãƒ†ã‚´ãƒª:** {product['category']}")

                                    with col2:
                                        st.metric("ä¾¡æ ¼", f"Â¥{product['price']:,}")
                                        if 'similarity_score' in product:
                                            st.metric("é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢", f"{product['similarity_score']:.3f}")
                        else:
                            st.info("æ¡ä»¶ã«åˆã†å•†å“ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    else:
                        st.error(f"æ¤œç´¢ã«å¤±æ•—ã—ã¾ã—ãŸ (Status: {response.status_code})")

                except Exception as e:
                    st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
            else:
                st.warning("Qdrant ã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“")


# ==================================================
# ãƒ‡ãƒ¼ã‚¿åˆ†æãƒšãƒ¼ã‚¸
# ==================================================
class DataAnalysisPage(PageManager):
    """ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒšãƒ¼ã‚¸"""

    def render(self):
        st.header("ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿åˆ†æã¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")

        # å¿…è¦ãªã‚µãƒ¼ãƒãƒ¼ã®ç¢ºèª
        status = self.status_manager.check_all_servers()
        required_servers = ['PostgreSQL', 'Redis']
        servers_ready = all("ğŸŸ¢" in status[server]["status"] for server in required_servers)

        if not servers_ready:
            st.warning("ãƒ‡ãƒ¼ã‚¿åˆ†æã«ã¯ PostgreSQL ã¨ Redis ã®æ¥ç¶šãŒå¿…è¦ã§ã™")

            # æ¥ç¶šçŠ¶æ³ã®è©³ç´°è¡¨ç¤º
            st.write("**ç¾åœ¨ã®æ¥ç¶šçŠ¶æ³:**")
            for server in required_servers:
                if server in status:
                    st.write(f"- {server}: {status[server]['status']}")
                else:
                    st.write(f"- {server}: çŠ¶æ…‹ä¸æ˜")
            return

        try:
            self._render_sales_analysis()
            self._render_customer_analysis()
            self._render_redis_statistics()
            self._render_advanced_analytics()
        except Exception as e:
            st.error(f"ãƒ‡ãƒ¼ã‚¿åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            st.code(traceback.format_exc())

    def _render_sales_analysis(self):
        """å£²ä¸Šåˆ†æ"""
        st.subheader("ğŸ’° å£²ä¸Šåˆ†æ")

        engine = sqlalchemy.create_engine(os.getenv('PG_CONN_STR'))

        col1, col2, col3 = st.columns(3)

        # ç·å£²ä¸Š
        total_sales = pd.read_sql("SELECT SUM(price * quantity) as total FROM orders", engine).iloc[0]['total']
        with col1:
            st.metric("ç·å£²ä¸Š", f"Â¥{total_sales:,.0f}")

        # å¹³å‡æ³¨æ–‡ä¾¡æ ¼
        avg_order = pd.read_sql("SELECT AVG(price * quantity) as avg FROM orders", engine).iloc[0]['avg']
        with col2:
            st.metric("å¹³å‡æ³¨æ–‡ä¾¡æ ¼", f"Â¥{avg_order:,.0f}")

        # æ³¨æ–‡æ•°
        order_count = pd.read_sql("SELECT COUNT(*) as count FROM orders", engine).iloc[0]['count']
        with col3:
            st.metric("ç·æ³¨æ–‡æ•°", f"{order_count:,}")

        # å•†å“åˆ¥å£²ä¸Š
        st.subheader("ğŸ“Š å•†å“åˆ¥å£²ä¸Š")
        product_sales = pd.read_sql("""
                                    SELECT product_name,
                                           SUM(price * quantity) as total_sales,
                                           COUNT(*)              as order_count,
                                           AVG(price * quantity) as avg_order_value
                                    FROM orders
                                    GROUP BY product_name
                                    ORDER BY total_sales DESC
                                    """, engine)

        col1, col2 = st.columns(2)

        with col1:
            st.write("**å£²ä¸Šé‡‘é¡**")
            st.bar_chart(product_sales.set_index('product_name')['total_sales'])

        with col2:
            st.write("**æ³¨æ–‡ä»¶æ•°**")
            st.bar_chart(product_sales.set_index('product_name')['order_count'])

        # è©³ç´°ãƒ†ãƒ¼ãƒ–ãƒ«
        st.write("**å•†å“åˆ¥è©³ç´°ãƒ‡ãƒ¼ã‚¿**")
        st.dataframe(product_sales, use_container_width=True)

        engine.dispose()

    def _render_customer_analysis(self):
        """é¡§å®¢åˆ†æ"""
        st.subheader("ğŸ‘¥ é¡§å®¢åˆ†æ")

        engine = sqlalchemy.create_engine(os.getenv('PG_CONN_STR'))

        customer_stats = pd.read_sql("""
                                     SELECT c.city,
                                            COUNT(c.id)                            as customer_count,
                                            COUNT(o.id)                            as total_orders,
                                            COALESCE(SUM(o.price * o.quantity), 0) as total_spent,
                                            COALESCE(AVG(o.price * o.quantity), 0) as avg_order_value
                                     FROM customers c
                                              LEFT JOIN orders o ON c.id = o.customer_id
                                     GROUP BY c.city
                                     ORDER BY total_spent DESC
                                     """, engine)

        col1, col2 = st.columns(2)

        with col1:
            st.write("**éƒ½å¸‚åˆ¥é¡§å®¢æ•°**")
            st.bar_chart(customer_stats.set_index('city')['customer_count'])

        with col2:
            st.write("**éƒ½å¸‚åˆ¥å£²ä¸Š**")
            st.bar_chart(customer_stats.set_index('city')['total_spent'])

        # é¡§å®¢åˆ†æè©³ç´°
        st.write("**éƒ½å¸‚åˆ¥è©³ç´°ãƒ‡ãƒ¼ã‚¿**")
        st.dataframe(customer_stats, use_container_width=True)

        # ä¸Šä½é¡§å®¢åˆ†æ
        top_customers = pd.read_sql("""
                                    SELECT c.name,
                                           c.city,
                                           c.email,
                                           COUNT(o.id)               as order_count,
                                           SUM(o.price * o.quantity) as total_spent
                                    FROM customers c
                                             JOIN orders o ON c.id = o.customer_id
                                    GROUP BY c.id, c.name, c.city, c.email
                                    ORDER BY total_spent DESC
                                    LIMIT 10
                                    """, engine)

        st.write("**ä¸Šä½é¡§å®¢ Top 10**")
        st.dataframe(top_customers, use_container_width=True)

        engine.dispose()

    def _render_redis_statistics(self):
        """Redisçµ±è¨ˆ"""
        st.subheader("ğŸ”´ Redis çµ±è¨ˆ")

        try:
            r = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)

            col1, col2, col3 = st.columns(3)

            with col1:
                active_sessions = len(r.keys('session:*'))
                st.metric("ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³", active_sessions)

            with col2:
                page_views = r.get('counter:page_views') or 0
                st.metric("ãƒšãƒ¼ã‚¸ãƒ“ãƒ¥ãƒ¼", f"{page_views:,}")

            with col3:
                search_count = r.llen('search:recent')
                st.metric("æ¤œç´¢å±¥æ­´æ•°", search_count)

            # Redisè©³ç´°çµ±è¨ˆ
            st.write("**Redisè©³ç´°çµ±è¨ˆ**")

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆinfo commandã®çµæœã‚’ãƒ‘ãƒ¼ã‚¹ï¼‰
            redis_info = r.info()

            col1, col2, col3, col4 = st.columns(4)

            with col1:
                used_memory = redis_info.get('used_memory_human', 'N/A')
                st.metric("ä½¿ç”¨ãƒ¡ãƒ¢ãƒª", used_memory)

            with col2:
                connected_clients = redis_info.get('connected_clients', 0)
                st.metric("æ¥ç¶šã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ•°", connected_clients)

            with col3:
                total_commands = redis_info.get('total_commands_processed', 0)
                st.metric("ç·ã‚³ãƒãƒ³ãƒ‰æ•°", f"{total_commands:,}")

            with col4:
                uptime_days = redis_info.get('uptime_in_days', 0)
                st.metric("ç¨¼åƒæ—¥æ•°", f"{uptime_days}æ—¥")

        except Exception as e:
            st.error(f"Redisçµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

    def _render_advanced_analytics(self):
        """é«˜åº¦ãªåˆ†æ"""
        st.subheader("ğŸ”¬ é«˜åº¦ãªåˆ†æ")

        engine = sqlalchemy.create_engine(os.getenv('PG_CONN_STR'))

        # æ™‚ç³»åˆ—åˆ†æï¼ˆã‚‚ã—order_dateãŒã‚ã‚Œã°ï¼‰
        try:
            daily_sales = pd.read_sql("""
                                      SELECT DATE(order_date)      as date,
                                             COUNT(*)              as order_count,
                                             SUM(price * quantity) as daily_sales
                                      FROM orders
                                      WHERE order_date IS NOT NULL
                                      GROUP BY DATE(order_date)
                                      ORDER BY date
                                      """, engine)

            if len(daily_sales) > 0:
                st.write("**æ—¥æ¬¡å£²ä¸Šæ¨ç§»**")
                st.line_chart(daily_sales.set_index('date')['daily_sales'])

                st.write("**æ—¥æ¬¡æ³¨æ–‡æ•°æ¨ç§»**")
                st.line_chart(daily_sales.set_index('date')['order_count'])
        except Exception:
            st.info("æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“")

        # å•†å“ã‚«ãƒ†ã‚´ãƒªåˆ†æ
        try:
            # å•†å“ãƒ†ãƒ¼ãƒ–ãƒ«ãŒã‚ã‚‹ã¨ä»®å®š
            category_analysis = pd.read_sql("""
                                            SELECT p.category,
                                                   COUNT(DISTINCT p.id)                   as product_count,
                                                   COALESCE(SUM(o.price * o.quantity), 0) as total_sales,
                                                   COALESCE(COUNT(o.id), 0)               as order_count
                                            FROM products p
                                                     LEFT JOIN orders o ON p.name = o.product_name
                                            GROUP BY p.category
                                            ORDER BY total_sales DESC
                                            """, engine)

            if len(category_analysis) > 0:
                st.write("**ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ**")
                st.dataframe(category_analysis, use_container_width=True)
        except Exception:
            st.info("ã‚«ãƒ†ã‚´ãƒªåˆ†æã¯åˆ©ç”¨ã§ãã¾ã›ã‚“")

        engine.dispose()


# ==================================================
# è¨­å®šãƒšãƒ¼ã‚¸
# ==================================================
class SettingsPage(PageManager):
    """è¨­å®šã¨ãƒ˜ãƒ«ãƒ—ãƒšãƒ¼ã‚¸"""

    def render(self):
        st.header("âš™ï¸ è¨­å®šã¨ãƒ˜ãƒ«ãƒ—")

        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        self._render_system_info()

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±
        self._render_performance_info()

        # Docker ã‚³ãƒãƒ³ãƒ‰
        self._render_docker_commands()

        # MCP ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæƒ…å ±
        self._render_mcp_endpoints()

        # ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
        self._render_troubleshooting()

        # ã‚¢ãƒ—ãƒªæƒ…å ±
        self._render_app_info()

    def _render_system_info(self):
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®è¡¨ç¤º"""
        st.subheader("ğŸ–¥ï¸ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±")

        col1, col2 = st.columns(2)

        with col1:
            st.write("**ç’°å¢ƒå¤‰æ•°:**")
            env_status = {
                "OPENAI_API_KEY": "è¨­å®šæ¸ˆã¿" if os.getenv('OPENAI_API_KEY') else "âŒ æœªè¨­å®š",
                "REDIS_URL"     : os.getenv('REDIS_URL', 'æœªè¨­å®š'),
                "PG_CONN_STR"   : "è¨­å®šæ¸ˆã¿" if os.getenv('PG_CONN_STR') else "âŒ æœªè¨­å®š",
                "ELASTIC_URL"   : os.getenv('ELASTIC_URL', 'http://localhost:9200'),
                "QDRANT_URL"    : os.getenv('QDRANT_URL', 'http://localhost:6333')
            }

            for key, value in env_status.items():
                if "âŒ" in str(value):
                    st.error(f"**{key}**: {value}")
                else:
                    st.success(f"**{key}**: {value}")

        with col2:
            st.write("**ã‚µãƒ¼ãƒãƒ¼æ¥ç¶šçŠ¶æ³:**")
            status = self.status_manager.check_all_servers()
            for server, state in status.items():
                if "ğŸŸ¢" in state["status"]:
                    st.success(f"**{server}**: æ¥ç¶šOK")
                    if "details" in state:
                        st.caption(f"è©³ç´°: {state['details']}")
                else:
                    st.error(f"**{server}**: æ¥ç¶šNG")
                    if "details" in state:
                        st.caption(f"è©³ç´°: {state['details']}")

    def _render_performance_info(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±ã®è¡¨ç¤º"""
        st.subheader("âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±")

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®ã‚µã‚¤ã‚º
        session_size = len(st.session_state)
        st.metric("ã‚»ãƒƒã‚·ãƒ§ãƒ³å¤‰æ•°æ•°", session_size)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±
        if hasattr(st, 'cache_data'):
            st.info("Streamlitã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã§ã™")

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆå¯èƒ½ã§ã‚ã‚Œã°ï¼‰
        try:
            import psutil
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            st.metric("ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡", f"{memory_mb:.1f} MB")
        except ImportError:
            st.info("è©³ç´°ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±ã«ã¯psutilãŒå¿…è¦ã§ã™")
        except Exception:
            st.info("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

    def _render_docker_commands(self):
        """Dockerç®¡ç†ã‚³ãƒãƒ³ãƒ‰ã®è¡¨ç¤º"""
        st.subheader("ğŸ³ Docker ç®¡ç†ã‚³ãƒãƒ³ãƒ‰")

        command_tabs = st.tabs(["èµ·å‹•", "åœæ­¢", "ãƒ­ã‚°ç¢ºèª", "ãƒ‡ãƒ¼ã‚¿ãƒªã‚»ãƒƒãƒˆ"])

        with command_tabs[0]:
            st.write("**ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èµ·å‹•:**")
            st.code("docker-compose -f docker-compose.mcp-demo.yml up -d redis postgres elasticsearch qdrant")

            st.write("**MCPã‚µãƒ¼ãƒãƒ¼èµ·å‹•:**")
            st.code("docker-compose -f docker-compose.mcp-demo.yml up -d redis-mcp postgres-mcp es-mcp qdrant-mcp")

            st.write("**å…¨ã‚µãƒ¼ãƒ“ã‚¹èµ·å‹•:**")
            st.code("docker-compose -f docker-compose.mcp-demo.yml up -d")

        with command_tabs[1]:
            st.write("**å…¨ã‚µãƒ¼ãƒ“ã‚¹åœæ­¢:**")
            st.code("docker-compose -f docker-compose.mcp-demo.yml down")

            st.write("**ãƒœãƒªãƒ¥ãƒ¼ãƒ å‰Šé™¤ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚‚å‰Šé™¤ï¼‰:**")
            st.code("docker-compose -f docker-compose.mcp-demo.yml down -v")

        with command_tabs[2]:
            st.write("**å…¨ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ­ã‚°:**")
            st.code("docker-compose -f docker-compose.mcp-demo.yml logs -f")

            st.write("**ç‰¹å®šã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ­ã‚°:**")
            st.code("docker-compose -f docker-compose.mcp-demo.yml logs -f redis-mcp")

        with command_tabs[3]:
            st.write("**ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å†æŠ•å…¥:**")
            st.code("uv run python scripts/setup_test_data.py")

            st.write("**å®Œå…¨ãƒªã‚»ãƒƒãƒˆ:**")
            st.code("""
# åœæ­¢ã—ã¦ãƒœãƒªãƒ¥ãƒ¼ãƒ å‰Šé™¤  
docker-compose -f docker-compose.mcp-demo.yml down -v

# å†èµ·å‹•
docker-compose -f docker-compose.mcp-demo.yml up -d

# ãƒ‡ãƒ¼ã‚¿å†æŠ•å…¥
uv run python scripts/setup_test_data.py
            """)

    def _render_mcp_endpoints(self):
        """MCPã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæƒ…å ±ã®è¡¨ç¤º"""
        st.subheader("ğŸŒ MCP ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ")

        mcp_endpoints = {
            "Redis MCP"        : "http://localhost:8000/mcp",
            "PostgreSQL MCP"   : "http://localhost:8001/mcp",
            "Elasticsearch MCP": "http://localhost:8002/mcp",
            "Qdrant MCP"       : "http://localhost:8003/mcp"
        }

        st.json(mcp_endpoints)

        # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
        st.write("**ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯:**")
        for name, url in mcp_endpoints.items():
            try:
                response = requests.get(url, timeout=3)
                if response.status_code == 200:
                    st.success(f"{name}: ğŸŸ¢ å¿œç­”OK")
                else:
                    st.warning(f"{name}: ğŸŸ¡ å¿œç­”ã‚ã‚Š (Status: {response.status_code})")
            except Exception:
                st.error(f"{name}: ğŸ”´ å¿œç­”ãªã—")

    def _render_troubleshooting(self):
        """ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®è¡¨ç¤º"""
        st.subheader("ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°")

        with st.expander("â“ ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•"):
            st.markdown("""
            **ğŸ”´ Redis æ¥ç¶šã‚¨ãƒ©ãƒ¼**
            - Dockerã‚³ãƒ³ãƒ†ãƒŠãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèª: `docker ps | grep redis`
            - ãƒãƒ¼ãƒˆ6379ãŒä½¿ç”¨ä¸­ã§ãªã„ã‹ç¢ºèª: `lsof -i :6379`
            - Redisè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª: `docker logs redis`

            **ğŸŸ¦ PostgreSQL æ¥ç¶šã‚¨ãƒ©ãƒ¼**
            - èªè¨¼æƒ…å ±ã‚’ç¢ºèª: `testuser/testpass`
            - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ã‚’ç¢ºèª: `docker-compose logs postgres`
            - æ¥ç¶šæ–‡å­—åˆ—ã®ç¢ºèª: `PG_CONN_STR`ç’°å¢ƒå¤‰æ•°

            **ğŸŸ¡ Elasticsearch æ¥ç¶šã‚¨ãƒ©ãƒ¼**
            - ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§: `docker stats`
            - Java heap sizeè¨­å®šã‚’ç¢ºèª: `ES_JAVA_OPTS=-Xms512m -Xmx512m`
            - ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ç¢ºèª: `curl http://localhost:9200/_cat/indices`

            **ğŸŸ  Qdrant æ¥ç¶šã‚¨ãƒ©ãƒ¼**
            - ã‚³ãƒ³ãƒ†ãƒŠã®èµ·å‹•çŠ¶æ³: `docker-compose ps qdrant`
            - ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯: `curl http://localhost:6333/`
            - ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ç¢ºèª: `curl http://localhost:6333/collections`

            **ğŸ¤– OpenAI API ã‚¨ãƒ©ãƒ¼**
            - APIã‚­ãƒ¼ã®è¨­å®šç¢ºèª: `.env`ãƒ•ã‚¡ã‚¤ãƒ«ã®`OPENAI_API_KEY`
            - ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã®ç¢ºèª: OpenAIãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
            - ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã®ç¢ºèª

            **ğŸ³ Dockeré–¢é€£ã‚¨ãƒ©ãƒ¼**
            - Docker Daemonã®èµ·å‹•ç¢ºèª: `docker info`
            - ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ã®ç¢ºèª: `docker system df`
            - ãƒãƒ¼ãƒˆç«¶åˆã®ç¢ºèª: `netstat -tulpn | grep :6379`

            **ğŸ’¾ ãƒ‡ãƒ¼ã‚¿é–¢é€£ã‚¨ãƒ©ãƒ¼**
            - ãƒ‡ãƒ¼ã‚¿åˆæœŸåŒ–: `scripts/setup_test_data.py`ã®å®Ÿè¡Œ
            - ãƒœãƒªãƒ¥ãƒ¼ãƒ ã®å†ä½œæˆ: `docker-compose down -v && docker-compose up -d`
            """)

        # è‡ªå‹•è¨ºæ–­æ©Ÿèƒ½
        with st.expander("ğŸ” è‡ªå‹•è¨ºæ–­"):
            if st.button("ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­ã‚’å®Ÿè¡Œ"):
                self._run_system_diagnosis()

    def _run_system_diagnosis(self):
        """ã‚·ã‚¹ãƒ†ãƒ è‡ªå‹•è¨ºæ–­"""
        st.write("**ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­ã‚’å®Ÿè¡Œä¸­...**")

        diagnosis_results = []

        # Dockerç¢ºèª
        try:
            import subprocess
            result = subprocess.run(['docker', '--version'],
                                    capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                diagnosis_results.append("âœ… Docker: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿")
            else:
                diagnosis_results.append("âŒ Docker: å•é¡Œã‚ã‚Š")
        except Exception:
            diagnosis_results.append("âŒ Docker: ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“")

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç¢ºèª
        status = self.status_manager.check_all_servers()
        for server, state in status.items():
            if "ğŸŸ¢" in state["status"]:
                diagnosis_results.append(f"âœ… {server}: æ¥ç¶šOK")
            else:
                diagnosis_results.append(f"âŒ {server}: æ¥ç¶šNG")

        # ç’°å¢ƒå¤‰æ•°ç¢ºèª
        required_env_vars = ['OPENAI_API_KEY', 'PG_CONN_STR']
        for var in required_env_vars:
            if os.getenv(var):
                diagnosis_results.append(f"âœ… {var}: è¨­å®šæ¸ˆã¿")
            else:
                diagnosis_results.append(f"âŒ {var}: æœªè¨­å®š")

        # çµæœè¡¨ç¤º
        for result in diagnosis_results:
            if "âœ…" in result:
                st.success(result)
            else:
                st.error(result)

    def _render_app_info(self):
        """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã®è¡¨ç¤º"""
        st.subheader("â„¹ï¸ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±")

        app_info = {
            "ãƒãƒ¼ã‚¸ãƒ§ãƒ³"    : "2.0.0-refactored",
            "ä½œæˆæ—¥"        : "2024-01-15",
            "æœ€çµ‚æ›´æ–°"      : "2025-01-28",
            "Python"        : "3.11+",
            "Streamlit"     : st.__version__,
            "ä½¿ç”¨æŠ€è¡“"      : ["Docker", "Redis", "PostgreSQL", "Elasticsearch", "Qdrant", "OpenAI API"],
            "ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£": "ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼è¨­è¨ˆ",
            "ä¸»è¦æ”¹å–„ç‚¹"    : [
                "1,100è¡Œã‹ã‚‰50è¡Œã®ãƒ¡ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã«å‰Šæ¸›",
                "ã‚¯ãƒ©ã‚¹ãƒ™ãƒ¼ã‚¹ã®è¨­è¨ˆ",
                "æ©Ÿèƒ½åˆ¥ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆ†å‰²",
                "ä¿å®ˆæ€§ãƒ»æ‹¡å¼µæ€§ã®å‘ä¸Š"
            ]
        }

        st.json(app_info)

        # ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±
        with st.expander("ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãƒ»è‘—ä½œæ¨©æƒ…å ±"):
            st.markdown("""
            **MIT License**

            Copyright (c) 2025 MCP Demo Application

            æœ¬ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ãŠã‚ˆã³é–¢é€£æ–‡æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä»¥ä¸‹ã€Œã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã€ï¼‰ã®ã‚³ãƒ”ãƒ¼ã‚’å–å¾—ã™ã‚‹
            ã™ã¹ã¦ã®äººã«å¯¾ã—ã€ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚’ç„¡åˆ¶é™ã«æ‰±ã†ã“ã¨ã‚’ç„¡å„Ÿã§è¨±å¯ã—ã¾ã™ã€‚

            **ä½¿ç”¨ã—ã¦ã„ã‚‹ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ–ãƒ©ãƒª:**
            - Streamlit (Apache License 2.0)
            - Pandas (BSD License)
            - Redis-py (MIT License)
            - SQLAlchemy (MIT License)
            - ãã®ä»–requirements.txtã«è¨˜è¼‰ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
            """)


# ==================================================
# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
# ==================================================
__all__ = [
    'DirectQueryPage',
    'DataAnalysisPage',
    'SettingsPage',
]

